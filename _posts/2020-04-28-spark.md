---
layout: post
title: Spark
subtitle: Distributed computing
photo: fireworks-761547.jpg
photo-alt: Spark
tags: [Spark, Python]
category: [Systems]
---


PySpark

## DataFrames

https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/#:~:text=1.,Excel%20sheet%20with%20Column%20headers.


## Hadoop

### Setup

### Setup on Ubuntu

Follow instructions on: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

Make sure to have Java installed:
```
sudo apt-get update
sudo apt install openjdk-8-jre
```
and that `JAVA_HOME` is set to the correct location in `hadoop-3.2.1\etc\hadoop\hadoop-env.sh`, this may be: `/usr/lib/jvm/java-8-openjdk-amd64`.

Also include the following are set, where username is your linux username:
```bash
export HDFS_NAMENODE_USER=username
export HDFS_DATANODE_USER=username
export HDFS_SECONDARYNAMENODE_USER=username
export YARN_RESOURCEMANAGER_USER=username
export YARN_NODEMANAGER_USER=username
```

Install PySpark with pip:
```bash
pip install pyspark
```


Start with: `start-dfs.sh` or `.start-all.sh` (assuming `$HADOOP_INSTALL/sbin` is in the path). 

### Access HADOOP dashboard

The Hadoop dashboard, which includes information on HDFS can be accessed via:
```http://localhost:9870```

### Access Spark dashboard

After starting a new Spark job with:
```http://localhost:4040/jobs/```

Further reading:
- `https://spark.apache.org/docs/latest/web-ui.html`


#### DAG analysis

When executing a job on Spark an execution plan using a DAG will automatically be created. For best performance, data should be kept in the pipeline and should minimise the amount of shuffling between nodes. Code that is executed in a poorly constructed way may lead to an execution plan that is sub-optimal.

After running a job you can display the DAG tree to analyse the query plan and adjust code to optimise query execution. To achieve this:
- Navigate to: `localhost:4040/jobs`
- Select the job you wish to analyse
- On the newly opened Job details page click the `DAG Visualization` dropdown and click on the image.
- This shows metrics for all stages of the DAG operation. 


### Setup on Windows

Doesn't appear to work for version 3.2.1

https://towardsdatascience.com/installing-hadoop-3-2-1-single-node-cluster-on-windows-10-ac258dd48aef

Configuration for Hadoop cluster:

%HADOOP_HOME%\etc\hadoop\hdfs-site.xml
%HADOOP_HOME%\etc\hadoop\core-site.xml
%HADOOP_HOME%\etc\hadoop\mapred-site.xml
%HADOOP_HOME%\etc\hadoop\yarn-site.xml

### HDFS setup
Create the directories:
```cmd
V:\hadoop\data\dfs\datanode
V:\hadoop\data\dfs\namenode
```

Edit `hdfs-site.xml`, for a single cluster the replication value is `1`:

```cmd
<configuration>

<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:///V:/hadoop/data/dfs/namenode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:///V:/hadoop/data/dfs/datanode</value>
</property>

</configuration>
```
### Starting Hadoop

```cmd
cd %HADOOP_HOME%
.\sbin\start-dfs.cmd
```

If you have the error Missing `server' JVM (Java\jre8\bin\server\jvm.dll.), copy the contents of C:/Java/java/jre8/bin/client into a new folder C:/Java/java/jre8/bin/client. 


## Hive

## Impala

## Yarn

