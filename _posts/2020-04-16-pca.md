---
layout: post
title: Principal Component Analysis
subtitle: PCA
photo: photo-of-blue-pink-and-green-led-light-775907.jpg
photo-alt: Clustering data
tags: [Sphinx, Python]
category: [Systems]
---

pg 269 Hands on machine learning

Often datasets contain many data dimensions, making analysis time intensive and complex. Determining which of these data features is pertinent to the problem at hand can also be difficult. Principal component analysis (PCA) is an unsupervised machine learning technique that determines the hyperplane that best separates the data.

 

The orthogonal unitvectors of this plane, known as _principal components_, describe the data directions that maximise the variance of data and hence the information content that is encoded. These principal components are linear combinations of the original dataset, ordered by their explained variance of the original data. Selecting the first few components after the PCA transform can reduce the amount of data dimensions whilst retaining much of the underyling encoded information.   

## Curse of dimensionality

As data dimensions increase the parameter space increases exponetially.

Average separation of two points in 2D space ~0.52.
Average separation of two points in 3D space ~0.66.
Average separation of two points in 1,000,000D space ~408.25.

Need many more data points to fully cover the space however this is computationally impossible.



## Dimensionality reduction

- Typically select dimensions such that the cumulative variance explain is 95% or select the elbow of the cumulatative variance explain distribution.
- For visualisation typically reduce to 2-3 dimensions as higher dimensions are difficult to visualise. 
- Due to its ability to reduce the data size whilst retaining information, PCA can also be used as a form of lossy compression. The MNIST dataset for instance can be reduced by ~80% from its original 784 features to 150 whilst retaining a 95% variance explain.





## Clustering data


## PCA in practice - Sklearn

- Before performing PCA, the data must first be centered such that the mean of each dimension is zero. Sklearn's `PCA` will automatically perform this scaling for you.
- The data however should also be normalised as otherwise dimensions with larger values will skew the fit. This can be performed by sklearn `StandardScaler`.

An example using the Iris dataset is shown below.

```python 
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
iris = datasets.load_iris()

X = iris.data
y = iris.target
labels = {i: name for i, name in enumerate(iris.target_names)}

from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y)

std_pca_pipe = make_pipeline(StandardScaler(), PCA())
std_pca_pipe.fit(X_train)
# Need to use named_steps['pca'] to get PCA from the pipeline
std_pca = std_pca_pipe.named_steps['pca']

```

- Note that the SVD solver for Sklearn's PCA algorithm needs to store the entire training set in memory. If the training set is too large then `IncrementalPCA` can be used which processes the data in minibatches. 