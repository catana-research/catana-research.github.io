---
layout: post
title: Model metrics
subtitle: Quantifying algorithmic performance
photo: building-clouds-facade-glass-panels-412842.jpg
photo-alt: Metrics
tags: [Models, Python]
category: [Analysis]
---

An important aspect in the development of models is determining which model or choice of hyperparmeters performs best. This cannot be achieved however without a suitable choice of model metric that can quantify modelling performance.

The choice of metric is dependent on the task 

### References
https://scikit-learn.org/stable/modules/model_evaluation.html

## Classification

True Positives (TP) and False Postives (FP)

- Accuracy
- Recall - TP/(TP + FP)
- Precision - TP/(TP + FP)
- F1 - 2/(1/precision + 1/recall)
- ROC - Receiver Operating Characteristic curve is a plot of the TP rate vs FP rate.
 Area Under the Receiver Operating Characteristic Curve

## Regression

- RMSE - l2 norm, euclidean distance measure. Preferred error measure. Can be senstive to outliers.
- MSE - RMSE without the square root. As the minimisation of the RMSE and MSE will give the same result, for efficiency MSE is sometimes used to reduce the required calculation. 
- MAE - l1 norm, Manhattan distance measure. Less sensitive to outliers.
- Max error
