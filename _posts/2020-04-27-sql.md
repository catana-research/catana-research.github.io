---
layout: post
title: SQL
subtitle: Developing with relational databases
photo: abstract-architectural-design-architecture-building-2276927.jpg
photo-alt: SQL
tags: [SQL]
category: [Systems]
---

Although Structured Query Language (SQL) is a relatively old technology, it is still remains a popular language for interfacing with databases. These relational databases store data in a row-column format and are best used for when the input data predictable and structured. 

New database instances are relatively easy to setup, which can range from comprehensive database infrastructure such as those provided by Microsoft SQL Server and Azure, to lightweight file-based instances such as SQLLite. 

## Developing in SQL

SQL is very different from procedural languages and requires a different mindset when developing algorithms. For instance looping over data to perform a manipulation or selection is extremely inefficient in SQL. Additionally, data is not stored in a single object but in one or more tables which must be queried together to extract the complete data.  

To demonstrate the functionality of SQL, consider the problem of storing data for energy readings.

### Creating a table

First lets create the table `Energy` for storing the data. This has a unique integer primary key `EntryID`, a date for the reading `ReadDate` (where the format is YYYY-MM-DD) and the energy reading for the date in kWh `Units` stored in a decimal format:
```sql
CREATE TABLE Energy(
    EntryID             INT IDENTITY(1,1),
    ReadDate            DATE,
    Units               DECIMAL(4, 2),
	
    PRIMARY KEY (EntryID),
)
```
A list of supported data types and their syntax can be found [here](https://www.w3schools.com/sql/sql_datatypes.asp).

Next lets add some data by using `INSERT`, specifying the columns and values we want to populate. Note because we made `EntryID` an `IDENTITY` column, it is automatically assigned on insertion:
```sql
INSERT INTO Energy (ReadDate, Units)
VALUES ('2020-01-01', 12.3), 
       ('2020-01-02', 9.3),
       ('2020-01-03', 11.3)
```
We can query the `Energy` table to see all the data stored using a `SELECT` query with the wildcard operator `*`:
```sql
SELECT * FROM Energy
```

This returns the following table:
```
EntryID	ReadDate	Units
1	    2020-01-01	12.3
2	    2020-01-02	9.3
3	    2020-01-03	11.3
```

We can apply filter selections using the `WHERE` statement, in addition to naming specific columns to restrict the data returned:
```sql
SELECT ReadDate, Units FROM Energy
	WHERE Units > 10.
```


### Aggregation
Inbuilt aggregation functions: `COUNT`, `MIN`, `MAX`, `SUM`, `AVG`, `STDDEV`

```sql
SELECT COUNT(*) FROM Star
  WHERE radius > 1;
```

### GroupBy
Aggregation can also be applied 

### Table Altering and Updates

So far we have stored data on energy usage but have no information on the cost associated with that use. To rectify this, let's create another table `EnergySupplier`, which contains information on the energy tariff and populate some data:
```sql
CREATE TABLE EnergySupplier(
    SupplierID          INT IDENTITY(1,1),
    SupplierName        VARCHAR(40),
    PricePerUnit        DECIMAL(4,2),
    StandingCharge      DECIMAL(4,2),    
    
    PRIMARY KEY(SupplierID)
)
GO

INSERT INTO EnergySupplier (SupplierName, PricePerUnit, StandingCharge)
    VALUES ('CheapEnergy', 11.4, 25.6),
           ('PremiumPower', 31.4, 50.4),
           ('GreenElectric', 13.4, 15.4)
```
where we used `GO` to separate the query into two transaction batches. 

Our `EnergySupplier` table now looks like this:
```
SupplierID	SupplierName	PricePerUnit	StandingCharge
1	CheapEnergy	11.40	25.60
2	PremiumPower	31.40	50.40
3	GreenElectric	13.40	15.40
```

Splitting the database into tables according to their data content is known as *normalisation*. Relationships are imposed between these tables to link the data with the addition of constraints to ensure data integrity. This is a good design practice that helps to reduce data duplication and simiplifies maintainance.

We now want to modify our existing `Energy` table to link to `EnergySupplier` so we can link tariff information. We could achieve this by deleting the existing table using  the `DROP` command however this will result in all our stored data being lost. Another option is to use `ALTER`, which enables the schema of an existing table to be modified:
```sql
ALTER TABLE Energy
    ADD SupplierID INT
```
The added column will default to `NULL` values, we can modify this data with an `UPDATE` statement. In this example we are with *CheapEnergy*, which we saw before had `SupplierID` = 1. We can explictly update the data in `Energy` to this new value with:  
```sql
UPDATE Energy
SET SupplierID = 1
```

A better method however would be to use a query to the `ElectricSupplier` table to get the correct `SupplierID` for the `Energy` table via a sub-query:
```sql
UPDATE Energy
SET SupplierID = (SELECT SupplierID FROM EnergySupplier WHERE SupplierName = 'CheapEnergy')
```
This enables more complex logic to be applied and avoids mistakes that can be introduced when hardcoding values into queries.

Finally, we are going to add a `FOREIGN KEY` constraint. This ensures that all new data added in `Energy` must match a `PRIMARY KEY` in `EnergySupplier`:
```sql
ALTER TABLE Energy
ADD FOREIGN KEY (SupplierID) REFERENCES EnergySupplier(SupplierID)
```

This protects us from data entry errors by in future, for example adding data for a `SupplierID` that does not exist:
```sql
UPDATE Energy
SET SupplierID = 10
WHERE EntryID = 1
```
throws the error:
```sql
The UPDATE statement conflicted with the FOREIGN KEY constraint "FK__Energy__Supplier__571DF1D5". The conflict occurred in database "data", table "dbo.EnergySupplier", column 'SupplierID'.
```

### Joins

Joins are incredibly powerful operations that produce a result that is a composite of data from two different tables. SQL is optimised for joins, enabling efficient calculation of operations that would typically be performed by a loop.

When specifying a join you must specify the two tables that are to be joined, the variables you want to use in the join and the type of join. There are several different types of join that can be applied. These can be thought of as a Venn diagram of the two sets of data `A` and `B`:
- `INNER JOIN`: Returns only rows that have data matching in both tables (`A AND B`).
- `LEFT OUTER JOIN`: All rows from the left table are kept and missing matches from the right table are replaced with NULL values (`A`).
- `RIGHT OUTER JOIN`: All rows from are kept and missing matches from are replaced with NULL values (`B`).
- `FULL OUTER JOIN`: All rows from both tables are kept (`A OR B`).


Now we have two separate tables containing our data we can use a `JOIN` to return data that combines their information. The syntax to join requires specifying which columns to join the data on.

```sql
SELECT * FROM Energy
	INNER JOIN EnergySupplier ON Energy.SupplierID = EnergySupplier.SupplierID
```
This returns 
```sql
EntryID	ReadDate	Units	SupplierID	SupplierID	SupplierName	    PricePerUnit	StandingCharge
1	    2020-01-01	12.30	1	        1	        CheapEnergy	        11.40	        25.60
2	    2020-01-02	9.30	1	        1	        CheapEnergy	        11.40	        25.60
3	    2020-01-03	11.30	1	        1	        CheapEnergy	        11.40	        25.60
```

We can reference tables more succinctly with aliases using the `AS` keyword:
```sql
SELECT * FROM Energy AS E
	INNER JOIN EnergySupplier AS ES ON E.SupplierID = ES.SupplierID
```

Be careful when joining on variables that are non-unique as this can lead to data blowup. For example joining two tables on two columns with 10 rows of the same value will result in a joined table with 10 x 10 = 100 rows.

### Views, Functions and Stored procedures

Common and complex queries used throughout can be encoded into callable functions. The type of function it is stored as depends on the behaviour of the underlying code:

`VIEW`: A snippet of code that returns a virtual table. This is useful for returning a `SELECT` of multiple tables.
`FUNCTION`: A snippet of code that is compiled and executed for every call, a result must be returned. 
`PROCEDURE`: A snippet of code that is compiled the first time it is executed. No data is required to be returned.  

For example...
```sql


```

You can then execute as follows:
```sql
EXEC prcMakeDealEntryTest   1, @TestDate='', ...
```

### Table variables

Sometimes a calculation is complex and requires storing an intermediate result. The *table variable* provides a method of persisting data in memory without having to write it to the database. An example: 
```sql
DECLARE @TempTable TABLE(LeftStr VARCHAR(MAX), RightStr VARCHAR(MAX))

INSERT INTO @TempTable
SELECT Arg1, Arg2 FROM TableName2
``` 


## Building an SQL framework

If you are considering controlling an SQL server instance purely with SQL scripts you may find it becomes difficult to maintain without a structure. One potential design is to group scripts by their type and name according to their function:

```bash
└───DBName
    │   BuildAll.sql
    |
    ├───ConfigurationData
    │       cfgConfiguration1.sql    
    |
    ├───Functions
    │       fncFunction1.sql    
    |
    ├───StoredProcedures
    │       prcStoredProcedure1.sql    
    |    
    ├───Tables
    │       Table1.sql
    |
    └───Views
        vwView1.sql
``` 

Where the directories contain the following:

- `ConfigurationData`: Scripts for populating static data in the database such as the defintion of different class attributes. Names are prefixed by 'cfg'.
- `Functions`: SQL function definitions. Names are prefixed by 'fnc'. 
- `StoredProcedures`: SQL stored procedure definitions. Names are prefixed by 'prc'.
- `Tables`: SQL table creation scripts. Names match that of their table.

The file `BuildAll.sql` contains defintions to execute the SQL scripts for deployment. This enables a deployment of the database architecture on a new system or the update of an existing structure. An example is included below:
```bash
-- BuildAll.sql
--   Runs all the required scripts to create Tables, StoredProcedures and Data.
--   Switch to SQLCMD mode prior to executing script: (Query > SQLCMD Mode in SSMS)

-- Adjust path to location of SQL scripts
:setvar path C:/MyFilePathToSQLScripts

USE DBName
GO

--------------------------------
-- Tables
--------------------------------
:r $(path)/Tables/Table1.sql
GO

--------------------------------
-- Stored procedures
--------------------------------
:r $(path)/StoredProcedures/prcFile.sql
GO

--------------------------------
-- Configuration data
--------------------------------
:r $(path)/ConfigurationData/cfgFile.sql
GO
```

### Guard tags

To ensure a script can be deployed on an existing system, it is useful to include checks within scripts that are not supposed to change during a deployment to stop them being altered: 
```sql
IF NOT EXISTS (SELECT * FROM SYS.OBJECTS WHERE OBJECT_ID = OBJECT_ID(N'[dbo].[Table1]') AND TYPE IN (N'U'))
BEGIN

	CREATE TABLE [dbo].[Table1](
        ...
	)

END
GO
```

Conversely, for files we want to be updated we require they are deleted before updating their definition:
```sql
IF EXISTS (SELECT * FROM sys.objects WHERE object_id = OBJECT_ID(N'[dbo].[prcStoredProcedure1]') AND type in (N'P', N'PC'))
DROP PROCEDURE [dbo].[prcStoredProcedure1]
GO

CREATE PROCEDURE [dbo].[prcStoredProcedure1]
    ...
AS 
BEGIN
    ...
END
```


### Optimisation

A poorly designed structure can lead to issues of performance and maintainability. When building a database structure is often good to consider:
- Initially normalising your design as much as possible. Afterwards combine tables where it makes sense in terms of performance and maintenance.
- Analyse the performance of your queries by inspecting the execution plan. Remove bottlenecks to your workflow by a redesign of queries or the underlying table structure.
- Ensure that the data types for your tables are suitable for the data requirements. Using a type that over provisions relative to data requirements wastes space and increases IO unnessarily, e.g. if you only are going to use strings of length 40 use a `VARCHAR(40)` instead of a `VARCHAR(MAX)` (42 bytes vs 8002 bytes). Conversely, do not underprovision. Using a type that does not have the required precision or length could result in data input errors or schema changes. Optimisations should be targeted at bottlenecks and large tables where performance is an issue.  
- Apply filters such as `WHERE` and restricting columns as soon as possible in your queries, this can significantly cut down on both I/O and CPU processing. 
- Consider whether table data is better represented in a flat format or as variable-value pairs. A flat structure is fixed, requiring a schema adjustment with changes to the data structure. A variable-value structure can accomodate new data components easily making it useful where the input data format is changing. Variable-value structures however can lead to a more complex design and sometimes less efficient queries. 




## SQLAlchemy

SQLAlchemy is Object Relational Database (ORM) that can use multiple different backends, such as SQL server and Postgres to persist data. Unlike the table structure interface of an SQL database, an ORM API inputs and outputs data in the form of objects, reducing the overhead of unpacking tabular data into an Object Oriented format. 


### Example

The SQL mapping to our Python object is defined by creating a Python object that derives from the `declarative_base` object. The member variables of this object are then mapped to columns within the specified `__tablename__`

```python
from sqlalchemy import Column, Integer, String, Float
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class Room(Base):
    __tablename__ = 'room'
    
    id = Column(Integer, primary_key=True)
    
    code = Column(String(36), nullable=False)
    size = Column(Integer)
    price = Column(Integer)
    longitude = Column(Float)
    latitude = Column(Float)

```

Data can then be extracted and input by instances of this class.